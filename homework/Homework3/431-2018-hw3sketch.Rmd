---
title: "Answer Sketch for Homework 3"
author: "431 Staff and Professor Love"
date: "`Due 2018-09-21, version `r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    number_sections: yes
  html_document:
    code_folding: show
    toc: yes
    number_sections: yes
---

\newpage

## R Setup

Here's the complete R setup we used.

```{r setup, message=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)

library(magrittr); library(tidyverse)
```

Then we read in the data set, which we'd stored in the project directory.

```{r}
LBWunicef <- read.csv("LBWunicef.csv") %>% tbl_df
```

We could use `glimpse` to take a look at the data...

```{r}
glimpse(LBWunicef)
```

Or we could just list the tibble, as a check on what we've done...

```{r}
LBWunicef
```

The data includes 180 nations, and information on `lbw.pct` and `least.dev` status.

\newpage

# Question 1

> How many nations have non-missing low birth weight percentage estimates?

There are `r length(LBWunicef$lbw.pct)` nations with non-missing low birth weight percentage estimates.

## Using `dim` or `nrow` and `summary`

We can use the `dim` function, or the `nrow` function to determine the number of rows in the `LBWunicef` data, and we can use the `summary` function to see if there are any missing values in the `LBWunicef` data:

```{r}
dim(LBWunicef)

nrow(LBWunicef)

summary(LBWunicef)
```

If there were any NA values in `lbw.pct`, the summary would indicate that. Since it doesn't, we must have `r nrow(LBWunicef)` nations with a value of `lbw.pct`.

## Using `favstats`

We need to figure out the total sample size, and the number of missing `lbw.pct` values. Perhaps we could use the `favstats` function from the `mosaic` package.

```{r q1a}
mosaic::favstats(~ lbw.pct, data = LBWunicef) 
```

And in fact, there are no missing values in the low birth weight percentage data.

## Using `skim` from `skimr`

```{r}
skimr::skim_with(integer = list(hist = NULL)) 
## did that just to leave out the sparkline histograms

skimr::skim(LBWunicef)
```

## Using `anyNA` and `length`

Alternatively, using the `%$%` version of the pipe available in the `magrittr` package, we could use:

```{r}
LBWunicef %$% anyNA(lbw.pct) 
## returns TRUE if there are any missing (NA) values
## in the lbw.pct variable

## to get the number of values in lbw.pct, could use:
LBWunicef %$% length(lbw.pct)
```

For more on piping like this, visit the Pipes section in *R for Data Science* at http://r4ds.had.co.nz/pipes.html. The `%$%` function is described there as "exploding" out the variables in a data frame so that you can refer to them explicitly.

# Question 2

> Which nations have the three largest low birth weight percentages? Are each of these considered by the UN to be "least developed" nations or not?

The three largest low birth weight percentages in the data are Mauritania (35%), Pakistan (32%), and India (28%). Of these three nations, only the troubled Northern African nation of Mauritania falls in the "least developed nations" category.

## Using `dplyr` and the tidyverse

We can use `dplyr`, specifically the `arrange` function, to show a tibble that has been sorted in descending order of `lbw.pct`. R Studio's cheat sheet for Data Transformation at https://www.rstudio.com/resources/cheatsheets/ is very helpful here.

```{r}
LBWunicef %>% arrange(desc(lbw.pct))
```

And, if we wanted to view just the first three rows, we could arrange and then slice...

```{r}
LBWunicef %>%
  arrange(desc(lbw.pct)) %>%
  slice(1:3)
```

## A fast, one-line alternative with `rank`

```{r}
## The fastest one-line alternative I know
LBWunicef[which(rank(LBWunicef$lbw.pct) > length(LBWunicef$lbw.pct) - 3),]
```

## `sort`, `which` and brute force

Clearly, we could solve this problem through simple brute force, inspecting the data until we find the largest values, and then associating them with Nations. The `sort` and `which` commands can help us here.

```{r}
LBWunicef %$% sort(lbw.pct)
```

OK. So the three largest values have `lbw.pct` greater than 27. How do we identify which nations those are?

```{r}
LBWunicef %$% which(lbw.pct > 27)
```

And now that we know which row numbers are the top 3, we can show all of the available data related to those three row numbers (including their names) using `slice` to identify specific rows in the data.

```{r}
LBWunicef %>% slice(c(73, 103, 122))
```

\newpage

# Question 3

> Create a histogram of the low birth weight percentages, then superimpose a normal density function with the same mean and standard deviation in red. Based on your plot, is the standard deviation or the inter-quartile range a more appropriate measure of variation in the low birth weight rates? Why?

Here's one approach.

```{r}
ggplot(LBWunicef, aes(x = lbw.pct)) +
  geom_histogram(fill = "wheat", col = "black", 
                 binwidth = 1) +
  stat_function(fun = function(x, mean, sd, n)
    n * dnorm(x = x, mean = mean, sd = sd),
    args = with(LBWunicef, 
                c(mean = mean(lbw.pct), 
                  sd = sd(lbw.pct),
                  n = length(lbw.pct))),
    col = "red", lwd = 1.5) +
  labs(title = "Low Birth Weight % across 180 Nations", 
       x = "Low Birth Weight Percentage", 
       y = "Number of Nations")
```

Clearly, the plot shows substantial right skew, so assuming a Normal model is not well justified. Thus, the standard deviation is less appropriate as a measure of spread than the interquartile range.

\newpage

# Question 4

> Create a normal Q-Q plot for the low birth weight percentage estimates. Would you say that the data are approximately Normally distributed, or not approximately Normally distributed? Justify your answer by interpreting what you see in your plot, and whatever summary statistics you deem to be useful in making your decision.

Again, the data are clearly right skewed, as indicated by the curve in the normal Q-Q plot. 

```{r, fig.height = 5, fig.width = 5}
ggplot(LBWunicef, aes(sample = lbw.pct)) +
  geom_qq() + geom_qq_line(col = "red", lwd = 2) + 
  labs(title = "Normal Q-Q plot of LBW percentages",
       subtitle = "across 180 nations")
```

## Using Numerical Summaries to Assess Normality

As usual, we should focus first on the plots to assess Normality, which might realistically have included a boxplot or violin plot along with the histogram and Normal Q-Q plot we've seen. Summary statistics should play a supporting role.

### Thinking about A Skewness Measure

```{r}
mosaic::favstats(~ lbw.pct, data = LBWunicef)
```

As for summary statistics, the mean (`r round(mean(LBWunicef$lbw.pct),2)`) is well to the right of the median (`r median(LBWunicef$lbw.pct)`), and, since the standard deviation is `r round(sd(LBWunicef$lbw.pct),2)`. So the skew~1~ value is also indicative of right skew, with skew~1~ = `r round(with(LBWunicef, ( mean(lbw.pct) - median(lbw.pct) ) / sd(lbw.pct)), 3)`, which is essentially the value we usually use as a minimum indicator of substantial right skew.

```{r}
LBWunicef %>%
  summarize(mean(lbw.pct), median(lbw.pct), sd(lbw.pct),
    skew1 = ( mean(lbw.pct) - median(lbw.pct) ) / 
                                             sd(lbw.pct) )
```

### Thinking about the Empirical Rule

We've already decided now that the data aren't symmetric enough for a Normal model to be a particularly good choice. If we wanted, we could also determine whether the Empirical Rule holds well for these data, and use that to help guide our understanding of whether the Normal model would work well (although at this point, that seems pretty settled.)

For instance, if a Normal model held, then about 68% of the nations would fall within two standard deviations of the mean. Is that true here?

```{r}
LBWunicef %>%
  count(mean_pm_1sd = lbw.pct > mean(lbw.pct) - sd(lbw.pct) & 
        lbw.pct < mean(lbw.pct) + sd(lbw.pct) )
```

In fact, 143/180 is `r round(100*143/180,1)`% of the nations that fall within 1 SD of the mean. That's higher than we would expect in data that followed a Normal distribution, so this pushes us slightly further in the direction we were already going when we just had the pictures - of concluding that the Normal model isn't a good choice for these data.

If a Normal model held, for instance, then about 95% of the data would fall within two standard deviations of the mean. Is that true here?

```{r}
LBWunicef %>%
  count(mean_pm_2sd = lbw.pct > mean(lbw.pct) - 2*sd(lbw.pct) & 
        lbw.pct < mean(lbw.pct) + 2*sd(lbw.pct) )
```

And 172/180 is `r round(100*172/180,1)`% of the nations that fall within 2 SD of the mean value of `lbw.pct`. That's pretty close to expectations, but, again, the 1 SD empirical rule doesn't hold so well.

### Thinking about Hypothesis Testing (Shapiro-Wilk Test)

A really, really bad idea is to use a hypothesis test to assess Normality. Such a test is essentially valueless without first looking at a plot of the data. But such tests are available. None are great, specifically because they only test for specific types of non-Normality, and most people can visualize several types of non-Normality simultaneously, making that (visualization) a much more powerful tool (even if it seems less "objective").

One of the simplest of such tests to run is the Shapiro-Wilk test of Normality. That test estimates a *p* value, something that's very easy to misinterpret. In the case of a Shapiro-Wilk test, if you see a *p* value that is less than a given value (the most common choice is 0.05), then that suggests that there is some evidence of non-Normality in the way the Shapiro-Wilk test tries to find it, or at least there's more evidence than if the *p* value were larger. The *p* value is a conditional probability, so it will always fall between 0 and 1. 


```{r}
LBWunicef %$% shapiro.test(lbw.pct)
```

Here, the *p* value is very small, which pushes us slightly further in the direction of concluding that the Normal model isn't a good choice for these data. 

Other hypothesis tests are available for assessing non-Normality. Again, none are great. In fact, I can't remember the last time I reported a Shapiro-Wilk test (or any other hypothesis test for non-Normality) in my practical work.

# Question 5

> Display an effective graph comparing the two development groups (least developed nations vs. all other nations) in terms of their percentages of low birth weight births. What conclusions can you draw about the distribution of low birth weight rates across the two development groups? Be sure to label your graph so it stands alone, and also supplement your graph with separate text discussing your conclusions.

Generally, the low birth weight percentages are higher in the nations which are least developed, but there is considerable overlap.

## Preliminaries: Creating a Factor

Before I build my plot, I'll create a new factor variable in the `LBWunicef` data, which I'll call `least_developed` and which will contain the levels No and Yes, for the original numeric 0 and 1.

```{r}
LBWunicef <- LBWunicef %>%
  mutate(least_developed = fct_recode(factor(least.dev), "Yes" = "1", "No" = "0"))
```

\newpage

Just as a sanity check, I'll be sure I've recoded appropriately with a frequency table:

```{r}
LBWunicef %>% count(least_developed, least.dev)
```

## A Comparison Boxplot (and Violin Plot)

Now, I'll build a comparison boxplot. I'll get a little fancy and create violin plots while I am at it.

```{r}
ggplot(LBWunicef, aes(x = least_developed, y = lbw.pct)) + 
  geom_violin(col = "darkgoldenrod") +
  geom_boxplot(aes(fill = least_developed), width = 0.3) + 
  guides(fill = FALSE) +
  coord_flip() +
  labs(title = "Low Birth Weight % by Least Developed Nation Status", 
       y = "Low Birth Weight %", 
       x = "Least Developed Nation, per UN Population Division") +
  theme_bw()
```

### Note: Making the Width of the Violin reflect the sample size

You can set the `scale` parameter to "count" in the `geom_violin()` call to adjust the violins to have areas that are scaled proportionally to the number of observations. Otherwise, they will all have the same area.

Here's an example of that for our data, which shows off the much larger group of No than Yes nations in terms of Least Developed status.

```{r}
ggplot(LBWunicef, aes(x = least_developed, y = lbw.pct)) + 
  geom_violin(col = "darkgoldenrod", scale = "count") +
  geom_boxplot(aes(fill = least_developed), width = 0.3) + 
  guides(fill = FALSE) +
  coord_flip() +
  labs(title = "Low Birth Weight % by Least Developed Nation Status", 
       y = "Low Birth Weight %", 
       x = "Least Developed Nation, per UN Population Division") +
  theme_bw()
```

So we see that there are more No than Yes nations.

### What if you wanted the boxplots to indicate the size of the data?

You could use `varwidth = TRUE` in the `geom_boxplot` call, like this:

```{r}
ggplot(LBWunicef, aes(x = least_developed, y = lbw.pct)) + 
  geom_violin(col = "darkgoldenrod") +
  geom_boxplot(aes(fill = least_developed), varwidth = TRUE) + 
  guides(fill = FALSE) +
  coord_flip() +
  labs(title = "Low Birth Weight % by Least Developed Nation Status", 
       y = "Low Birth Weight %", 
       x = "Least Developed Nation, per UN Population Division") +
  theme_bw()
```

That approach makes the width (so height in this case, because we've flipped the coordinates) of the boxplot proportional to the square root of the sample size. So we see that there are more No than Yes.

## Another Reasonable Choice: Faceted Histograms

You could certainly have built a set of faceted histograms instead, but ideally, you'd have them arranged so that the distributions were easy to compare (the two histograms on top of each other, as these boxplots are, rather than just plotted next to each other.) That's part of the reason I flipped those boxplots. Here's our attempt.

```{r}
ggplot(LBWunicef, aes(x = lbw.pct, fill = least_developed)) + 
  geom_histogram(binwidth = 1, col = "white" ) +
  facet_grid(least_developed ~ ., labeller = "label_both") +
  guides(fill = FALSE) +
  labs(title = "Low Birth Weight % by Least Developed Nation Status", 
       y = "Number of Nations", 
       x = "Low Birth Weight %") +
  theme_bw()
```

This does convey a bit more effectively that the "least developed" nations comprise one-quarter (45/180) of the total set of nations, but I think on the whole I prefer the boxplot here.

\newpage

# Question 6

> Read the Introduction and Chapter 1 of Nate Silver's *The Signal and the Noise*. One possible takeaway, particularly from the Introduction, suggested, for example in a review by Jonah Sinick, might be that increased access to information can do more harm than good. 

> Tell us about an example in your own field/work/experience where a "surplus" of information made (or makes) it easier for people dealing with a complex system to cherry-pick information that supports their prior positions. What were the implications of your example in terms of lessons that can be learned? If you can connect your example to some of the lessons described in the Chapter 1 discussion of the failure to predict the 2008 catastrophe on the US economy, that would be welcome.

> Please feel free to supply as many supporting details as are useful to you in relating the story. An appropriate response to Question 6 will use complete English sentences with proper grammar and syntax, will cite a link or two to a Web URL or other published work, and be between 200 and 400 words long.

We don't write answer sketches for essay questions. We'll gather a few of the more interesting and enlightening responses, and share de-identified excerpts with the group after grading.

# Question 7

> Generate a "random" sample of 75 observations from a Normal distribution with mean 100 and standard deviation 10 using R. The `rnorm` function is likely to be helpful. Now, display a normal Q-Q plot of these data, using the `ggplot2` package from the `tidyverse`. How well does the Q-Q plot approximate a straight line? 

> Repeat this task for a second sample of 150 Normally distributed observations, again with a mean of 100 and a standard deviation of 10. Then repeat it again for samples of 25 and 225 Normally distributed observations with a different mean and variance. Which of the four Q-Q plots you have developed better approximates a straight line and what should we expect the relationship of sample size with this phenomenon to be?

## Why are there two answers here?

Because Dr. Love made a mistake. 

- Option 1 below describes a correct answer to the question, as it was asked.
- Option 2 below describes a correct answer to the question, if you ignore the key phrase "with a different mean and variance." which is what Dr. Love did in writing up this sketch initially, because he is sometimes no smarter than a computer.

We won't penalize you if your answer yields the same story as either Option 1 or Option 2.

## Option 1 - Answering the Question that we actually asked

We're going to first draw a random sample of 75 observations from a Normal distribution with mean 100 and standard deviation 10.

```{r}
set.seed(2018)
sample_75 <- rnorm(n = 75, mean = 100, sd = 10)
```

Then we'll put that sample into a tibble.

```{r}
q7a <- tbl_df(sample_75)
```

Now we'll draw a Normal Q-Q plot of those data.

```{r}
ggplot(q7a, aes(sample = sample_75)) + 
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "Normal Q-Q plot of 75 sampled observations")
```

Now, we'll do this again for a new sample of 150 observations, also drawn from a Normal distribution with mean 100 and standard deviation 10.

```{r}
sample_150 <- rnorm(n = 150, mean = 100, sd = 10)
q7b <- tbl_df(sample_150)

ggplot(q7b, aes(sample = sample_150)) +
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "Normal Q-Q plot of 150 sampled observations")
```

Next, we'll do this again for samples of first 25 and then 225 observations from a Normal distribution with a different mean (we'll use 400) and standard deviation (we'll use 100)

```{r}
sample_25 <- rnorm(n = 25, mean = 400, sd = 100)
q7c <- tbl_df(sample_25)

sample_225 <- rnorm(n = 225, mean = 400, sd = 100)
q7d <- tbl_df(sample_225)
```

OK. So now we have all four samples. Let's put the plots all together in a single figure.

```{r}
plot1 <- ggplot(q7c, aes(sample = sample_25)) +
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "25 observations from N(400, 100)")

plot2 <- ggplot(q7a, aes(sample = sample_75)) +
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "75 observations from N(100, 10)")

plot3 <- ggplot(q7b, aes(sample = sample_150)) +
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "150 observations from N(100, 10)")

plot4 <- ggplot(q7d, aes(sample = sample_225)) +
  geom_qq() + geom_qq_line(col = "red") +
  labs(title = "225 observations from N(400, 100)")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

All four of these plots show fairly modest deviations from what we would expect a Normal distribution to look like, usually in terms of showing a few outlying values.

With larger sample sizes, there's **no real reason** to assume that the plots will improve substantially in terms of eliminating outliers, in fact. Once we have at least 25 points (as in all of these cases) it appears that the results are fairly reasonable (in terms of suggesting that a Normal approximation is generally valid) in each of these plots.

## Option 2 - Ignoring the "with a different mean and variance" bit.

From a coding perspective, I'm just looking for you to properly draw a random sample from a Normal distribution and then produce the necessary plots. 

Note that you either want to use four different random seeds here, and build each sample separately, or build one long set of 475 samples `(75 + 150 + 25 + 225 = 475)` to cover all four needs, and then split the group of 475 values accordingly.

Building four separate samples might look like this:

```{r build4samples, eval=FALSE}
set.seed(43101)

x <- rnorm(n = 75, mean = 100, sd = 10) 
  samp1 <- data_frame(value = x, grp = rep("S-75", 75))

x <- rnorm(n = 150, mean = 100, sd = 10) 
  samp2 <- data_frame(value = x, grp = rep("S-150", 150))

x <- rnorm(n = 25, mean = 100, sd = 10) 
  samp3 <- data_frame(value = x, grp = rep("S-25", 25))

x <- rnorm(n = 225, mean = 100, sd = 10) 
  samp4 <- data_frame(value = x, grp = rep("S-225", 225))

q7_first_try <- bind_rows(samp1, samp2, samp3, samp4)
rm(samp1, samp2, samp3, samp4, x) # drop these vectors
```

But what I actually did was build a single set of 475 values, and then split them, using this code:

```{r}
set.seed(20180921)

big.sample <- rnorm(n = 475, mean = 100, sd = 10)

big.grp <- c(rep("n = 75", 75), rep("n = 150", 150), 
             rep("n = 25", 25), rep("n = 225", 225))

q7_data <- data_frame(value = big.sample, grp = big.grp)

rm(big.sample, big.grp) # we won't need those vectors again
```

So, now we are ready to build the four Normal Q-Q plots.

```{r, fig.height=5, fig.width = 5}
ggplot(q7_data, aes(sample = value, col = grp)) +
  geom_qq(size = 2) + geom_qq_line() +
  guides(color = FALSE) +
  facet_wrap(~ grp) +
  labs(title = "Normal Q-Q Plots for Simulated Normal Data") +
  theme_bw()
```

Again, all four of these plots show fairly modest deviations from what we would expect a Normal distribution to look like, usually in terms of showing a few outlying values.

With larger sample sizes, there's **no real reason** to assume that the plots will improve substantially in terms of eliminating outliers, in fact. Once we have at least 25 points (as in all of these cases) it appears that the results are fairly reasonable (in terms of suggesting that a Normal approximation is generally valid) in all of these plots.

